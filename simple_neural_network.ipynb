{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Anatomy of Computer Nueral Network**\n",
    "\n",
    "üïØÔ∏è _'An technician's job is not to simply use a tool and become slave of it, but to break the tool and build it from scratch in order to make it serve.'_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîñ **References**\n",
    "- This live playbook is inspired by the work of Andrew Trask's book: grokking Deep learning.\n",
    "- Narrative and use cases are different based upon my own blog's and experience.\n",
    "\n",
    "üìù **Author**: Reeshabh Choudhary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ù Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An invention find its inception in philosphy first and then it takes form in physical reality. It was the philosphical understanding of emptiness or void which laid the ground work for the mathematical adaptation of the number zero. Humans have always been fascinated with finding explcit expression to the implicit ideas in their mind. Like grammar of a language is an explicit discovery of the rules of the langugae. Mathematics is a tool based on series of ideas and their relationship with each other.\n",
    "\n",
    "Galileo Galilei famously wrote: \"_Philosophy is written in this grand book, the universe, which stands continually open to our gaze. But the book cannot be understood unless one first learns to comprehend the language and read the letters in which it is composed. It is written in the language of mathematics, and its characters are triangles, circles, and other geometric figures without which it is humanly impossible to understand a single word of it; without these, one wanders about in a dark labyrinth_\". \n",
    "\n",
    "Neural Networks are no differnt. The concept of neural networks draws inspiration from the intricate information processing of human neurons and its implementations are based upon differnet mathematical expressions which reflect its decison making ability. Consider this scenario: \n",
    "\n",
    "There is a football match being organized in your locality. Your favorite team is playing the match and you are trying to _choose_ if you want to attend the match or not. Your pending _choice_ lies on three factors: \n",
    "- Is it a weekday or weekend?\n",
    "- Are your friends going to join you?\n",
    "- Is your favorite player going to play the match?\n",
    "\n",
    "Now, these three factors can be represented by three variables say x1, x2 and x3 and the responses can be captured in binary format. 'Yes' will be marked as 1 and 'No' will be marked as 0.  In total 8 scenarios are possible, however the outcome is also in binary i.e. either you will go the football match (1) or you will not go (0). \n",
    "\n",
    "Suppose, match is on weekend but you do not want to go without your friends. But if your favorite player is going to play the match, you may probably consider going without your friends as well even though it is a weekday. \n",
    "\n",
    "> _What is Choice? It is the process of selecting most probable possibilities from a limited set of options._\n",
    "\n",
    "What we are doing here is considering the weight (w1, w2 and w3) each of the three factors press in the choice making to reach a final choice. And this can be well represented in a **perceptron model** proposed by Frank Rosenblatt in 1958, which aimed to simplify neural behavior to essential mathematical operations. The perceptron model does not quite mimic the brain functionality but it is one of the basic expressions of choice making process we employ in real-life secnarios.\n",
    "\n",
    "The perceptron model equation can be expressed as:\n",
    "\n",
    "$$\n",
    "f(x) = \n",
    "\\begin{cases} \n",
    "1, & \\text{if } w \\cdot x + b > threshold, \\\\\n",
    "0, & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "w \\cdot x = \\sum_{i=1}^m w_i x_i\n",
    "$$\n",
    "\n",
    "Here, _w_ denotes a vector of real valued weights, _m_ is the number of inputs to the perceptron, and _b_ is the bias. For now do not worry about the terminology as we will be doing exercises to cover them. \n",
    "\n",
    "---\n",
    "\n",
    "üîç We have used the term '_model_' here? So what exactly **_modelling_** means?\n",
    "Modelling refers to an abstract representation that captures essential behaviors or relationships, not necessarily physical form. For instance, consider different car models from differnt brands or companies being displayed at Auto Expo. Since all the models of a particular company can not be displayed, a car making company choses a model  which best represents the company's overall charachteristics, philosophy and design principles. Likewise, sceintific models articulate a concept that defines the general charachteristics of the environment it is trying to mimic. Although, the natural world scenario is quite chaotic with different parameters at play, a model tries to approimate useful predictions. A model is often based on certain assumptions and simplifications. It helps to manipulate and understand representaion of the phenomenon without observing the phenomenon itself.\n",
    "\n",
    "Scientific models require a mathematic approach to generalize and understand a phenomenon through a set of observations (_data_).\n",
    "\n",
    "---\n",
    "> A perceptron is a fundamental building block of artificial neural networks and serves as a simple model of information construction from sensory units. A perceptron receives multiple binary inputs, denoted as x1, x2, x3, ..., where each can be either 0 or 1 and produces a single binary output. Additionally, there are corresponding weights w1, w2, w3, ... associated with each input. The weights represent the importance or significance of each input in influencing the perceptron's output. The perceptron computes a weighted sum of the inputs and weights. This is done by multiplying each input by its corresponding weight and summing up these weighted values. The perceptron then compares the weighted sum to a threshold value. The threshold is a parameter of the perceptron that determines the point at which the perceptron will \"fire\" and produce an output of 1. If the weighted sum is greater than or equal to the threshold, the perceptron outputs 1; otherwise, its output is 0. By varying the weights and the threshold, we can get different models of decision-making. \n",
    "\n",
    "\n",
    "üëÜThe example scenario presented above is very simplistic one and falls under binary classification problem. However, in real-life the datasets can be in texts, images, videos, etc. \n",
    "Since, computers can‚Äôt digest texts or images or other formats of data, it becomes essential to represent data in numerical format in Machine Learning (ML). ML models need input to be _transformed_ or _encoded_ into numbers. And to do so we seek shelter in _Linear Algebra_!\n",
    "\n",
    "Let us look a small sample dataset to better our understanding.\n",
    "\n",
    "| Subject | Height (cm) | Weight (kg) | Age |\n",
    "|---------|-------------|-------------|-----|\n",
    "| Person 1| 170         | 80          | 30  |\n",
    "| Person 2| 160         | 75          | 20  |\n",
    "| Person 3| 158         | 78          | 25  |\n",
    "\n",
    "Each dataset resembles a table-like structure consisting of rows and columns, where each row represents observations, and each column represents features/variables. And a table like structure is represented using Matrices in Linear Algebra and it is represented using arrays  in computers. \n",
    "\n",
    "- **_Scalars_** are single numerical values that represent a single feature or data point. They are simple and easy to understand, as they only represent one piece of information. In above dataset, each individual height and weight is a scalar. For example, the height of Person 1 (170 cm) is a scalar value, and the weight of Person 2 (75 kg) is also a scalar value. Each scalar represents a single attribute of a person.\n",
    "\n",
    "- **_Vectors_** are 1-dimensional arrays that can represent multiple features together. Each element in the vector corresponds to a specific feature or attribute. For our example dataset, we can create three vectors to represent the dataset in form of Height, Weight and Age.\n",
    "\n",
    "üìå **NOTE:** The very popular Python library _NumPy_ is just a wrapper library for representing array of arrays and equipped with matrix oriented functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ **Purpose: The aim of this article is to understand how a neural network can be developed from scratch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setting up Network ü•Ö"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üé¢ Prediction is always based on input. We do this activity commonly in daily life. For example, if there is lot of humidity, we predict rain around the corner. Based upon new government regulations such as tobacco ban for instance, we predict the rise and fall of stock prices of a listed company selling tobacco based product. In India, ITC is one such company whose stock price is always the talk of the town after a financial budget.\n",
    "\n",
    "For a neural network to predict, it must process the data points passed to it as set of observations or _input_. But, it is difficult to process information and output predictions without any knowledge. So, neural network assigns weight to the information recieved. This is what we did when we discussed the football match decison making scenario. We assigned weights to input data points and based upon that we can try to predict the final outcome. \n",
    "\n",
    "> _More data points we expose our network, more it optimizes its weight for reaching the desired output. It is an ever continuous process where weights keep evolving._ üß†\n",
    "\n",
    "ü•ú In a nutshell, a neural network makes predictions based upon the weight given to the input data points. If a certain input plays a dominant role in the outcome, it is given larger weight and conversely smaller weight is assigned if input's role is less dominant. \n",
    "\n",
    "‚öñÔ∏è To represent this understanding mathematically, we would _scale_ the input corresponding to its weight. _Scaling_ is nothing but simple multiplication. Based upon weight, the input data point si scaled up or down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simplest Neural network with single input datapoint. \n",
    "\n",
    "ü¶æ Enough of talks and theories, now we get our hand dirty and put whatever we learnt so far into practice. We start from a bare minimal neural network and then we will build on top of it as we learn more concepts. \n",
    "\n",
    "üî≠ One common observation we apply in our daily lives is to categorize individuals in adult(1) or child(0) category based upon parameters their height, weight, body shape, face, etc. Let us perform this exercise with simplest parameter, say height. \n",
    "\n",
    "üíâ So, we narrow down the goal of a simple neural network to predict whether a person is adult or child based upon the person's height as an input factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü´è **Assumptions:**\n",
    "- Values closed to 1 will be classified as adult and closer to 0 will be classified as child. A classic binary classification problem.\n",
    "- We start with an arbitrary weight value i.e. 0.15 with a generalized assumption that person below 4ft would be considered a child and above 4ft would be considered an adult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# height of individuals in 'ft'\n",
    "person_height = [5.5, 2.5, 6.4, 7, 2.3]\n",
    "\n",
    "# We start with an arbitary weight value\n",
    "weight = 0.15\n",
    "\n",
    "def neural_network(input, weight):\n",
    "    prediction = input * weight\n",
    "    return prediction\n",
    "\n",
    "input = person_height[0]\n",
    "pred = neural_network(input, weight)\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> For the first input passed to our network, it predicts that there is 82.5% chance that the person is an adult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü´∏Raising complexity: Multiple Input Points\n",
    "\n",
    "Not bad for starters, however, real world scenarios rarely involve single input data point. To do a prediction, every single input parameter's weight and value will be considered in this scenario. This activity is called calculating _Weighted Sum_.\n",
    "\n",
    "For a single neuron, the weighted sum is calculated as:\n",
    "\n",
    "$$\n",
    "z = \\sum_{i=1}^n (x_i \\cdot w_i) + b\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$$\n",
    "x_i: \\text{The $i$-th input feature (e.g., height, weight, etc.)} \\\\\n",
    "w_i: \\text{The weight associated with the $i$-th input feature (learned by the network)} \\\\\n",
    "b: \\text{The bias term (an additional constant that shifts the output, also learned by the network)} \\\\\n",
    "n: \\text{The total number of inputs to the neuron}\n",
    "$$\n",
    "\n",
    "\n",
    "üìè In real world, different input points when represented mathematically will be on different scale. For example, A person's age may vary from 0-75 years, however his height may vary from 2ft to 7ft and weight may vary from 25lbs to 250lbs, etc. \n",
    "\n",
    "‚öñÔ∏è Problem starts when we start treating these input parameters on the same scale. Remember, input is scaled by the weight associated. This may cause the network to place undue importance on features with larger numerical values, not because they are more important, but because of their scale. The larger values (like weight and age) will dominate the smaller ones (like height) in computations like the weighted sum.\n",
    "\n",
    "üñêÔ∏è Hence, it is highly recommended to bring the input parameters on same scale before exposing them to network and the process is called **_Normalization_**. It ensures that each input factor contributes proportionally based on its actual importance rather than its scale. \n",
    "\n",
    "Generally, normalized value of an input value is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{Normalized Value} = \\frac{\\text{Value} - \\text{Min Value}}{\\text{Max Value} - \\text{Min Value}}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚è≠Ô∏è Now, we capture more than one input points. Apart from height of individuals, we also capture their weight, age and hair greying scale and based upon these inputs and given weights wrt inputs, we try to predict the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Sum Calculation\n",
    "def w_sum(a, b):\n",
    "    if (len(a) == len(b)):\n",
    "        output = 0\n",
    "    for i in range(len(a)):\n",
    "        output += (a[i] * b[i])\n",
    "    return output\n",
    "\n",
    "# Normalization\n",
    "def normalize_inputs(input_data, min_values, max_values):\n",
    "    return [(input_data[i] - min_values[i]) / (max_values[i] - min_values[i]) for i in range(len(input_data))]\n",
    "\n",
    "# Setting up Neural network\n",
    "def neural_network(input, weights):\n",
    "    pred = w_sum(input, weights)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, a library such as numPy can be used to calculated weighted sum:\n",
    "\n",
    "`\n",
    "def neural_network(input, weights): \n",
    "    pred = input.dot(weights)\n",
    "    return pred\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initializing arbitary weights w.r.t input values\n",
    "weights = [0.15, 0.01, 0.02, 0.5]\n",
    "# defining input parameters\n",
    "person_height = [5.5, 2.5, 6.4, 7, 2.3]\n",
    "person_weight = [150, 30, 180, 220, 25]\n",
    "person_age = [25, 4, 30, 35, 3]\n",
    "person_greying = [0.2, 0.0, 0.1, 0.3, 0.0]\n",
    "# Min values for [height, weight, age, hair greying]\n",
    "min_values = [2.3, 25, 3, 0.0]\n",
    "# Max values for [height, weight, age, hair greying]\n",
    "max_values = [7.0, 250, 100, 1.0]\n",
    "# First set of datapoints\n",
    "input = [person_height[0], person_weight[0], person_age[0], person_greying[0]]\n",
    "normalized_input = normalize_inputs(input, min_values, max_values)\n",
    "print(\"Normalized Input:\", normalized_input)\n",
    "# Passing normalized input to the network\n",
    "pred = neural_network(normalized_input, weights)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÅÔ∏è As other factors come into play, the prediction value has changed significantly. Now it is just a raw score rather than a probability or prediction.\n",
    "\n",
    "üìå **Note:** So far weights are constant. We are not adjusting anything or considering any feedback or validating outputs. We will come to that later. Our focus is right now to set up a network first and later we shall make it smarter by adjusting weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü´∏Raising complexity: multiple inputs and multiple outputs\n",
    "\n",
    "Since, we have multiple outcomes to predict and associated weights w.r.t to the outcomes and input parameters are defined, we will deal with a 2-D array. The underlying process remains same but will have to do vector-matrix multiplication of each input parameter wrt to weight assigned to it in reference to an output parameter.\n",
    "\n",
    "- For example: Input parameter 'height' has '0.15' weight associated to predict 'Whether individual is adult or not?' and '0.2' weight associated to predict 'Overall Health score (1-100)'.\n",
    "\n",
    "üîë _While making three independent predictions, the network will calculate three independent weighted sums of the input._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "def normalize_inputs(input_data, min_values, max_values):\n",
    "    return [(input_data[i] - min_values[i]) / (max_values[i] - min_values[i]) for i in range(len(input_data))]\n",
    "\n",
    "# Weighted Sum function\n",
    "def w_sum(a, b):\n",
    "    if len(a) == len(b):\n",
    "        output = 0\n",
    "        for i in range(len(a)):\n",
    "            output += a[i] * b[i]  # Weighted sum of inputs\n",
    "    return output\n",
    "\n",
    "# Vector-Matrix multiplication function which calculates the weighted sum of inputs w.r.t to weights\n",
    "def vect_mat_mul(input, weights):\n",
    "    if len(input) == len(weights[0]):\n",
    "        output = [0, 0]  # Adjusted to 2 outputs\n",
    "        for i in range(len(weights)):  # Loop for each output\n",
    "            output[i] = w_sum(input, weights[i])  # Calculate weighted sum for each output\n",
    "    return output\n",
    "\n",
    "# Neural network\n",
    "def neural_network(input, weights):\n",
    "    pred = vect_mat_mul(input, weights)\n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing arbitary weights w.r.t input values\n",
    "weights = [\n",
    "    [0.15, 0.01, 0.02, 0.5],  # Weights for adult probability output\n",
    "    [0.2, 0.005, 0.01, 0.4]  # Weights for overall health score output\n",
    "]\n",
    "\n",
    "# defining input parameters\n",
    "person_height = [5.5, 2.5, 6.4, 7, 2.3]\n",
    "person_weight = [150, 30, 180, 220, 25]\n",
    "person_age = [25, 4, 30, 35, 3]\n",
    "person_greying = [0.2, 0.0, 0.1, 0.3, 0.0]\n",
    "# Min values for [height, weight, age, hair greying]\n",
    "min_values = [2.3, 25, 3, 0.0]\n",
    "# Max values for [height, weight, age, hair greying]\n",
    "max_values = [7.0, 250, 100, 1.0]\n",
    "input = [person_height[0], person_weight[0], person_age[0], person_greying[0]]\n",
    "normalized_input = normalize_inputs(input, min_values, max_values)\n",
    "\n",
    "# Output from neural network\n",
    "prediction = neural_network(normalized_input, weights)\n",
    "print(\"Predicted Adult Probability and Health Score:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ Notice that predicted Adult probablity is the same as calculated earlier, which gives proper evidence that our network is predicting output independently.\n",
    "\n",
    "Also, if we want the health score prediction to be represented in a different range (e.g., 0 to 100 for better interpretability), we can scale it accordingly. We just have to apply Apply a simple scaling transformation to map the values:\n",
    "$$\n",
    "Health¬†Score¬†(0¬†to¬†100)=Prediction√ó100\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Note:** Prediction of a neural network is being calculated by taking dot prodct of two vectors namely _input_ and _weight_ . A dot product of two vectors represents similarity between the two vectors. When the product is large then the two vectors are aligned or pointing in similar dorection and when product is small then the vectors are unrelated or pointing in opposite direction. Neural networks adjust weights during training to maximize this similarity for correct predictions\n",
    "\n",
    "üìå **Note:** We are currently providing just one training example as input. We continue to do so for a while until we build upon the concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü•∑ Hidden Layers\n",
    "\n",
    "So far, we have been dealing with patterns which are very simplistic, however, real-life scenarios are far more complex.\n",
    "\n",
    "> üî®In essence, a neural network is a tool which looks for correlation between input and output datasets.\n",
    "However, in real world there will hardly be situations where input data will directly corelate with the output dataset. And just one-layered neural network won't be enough to understand such relations. \n",
    "\n",
    "üîé In order to find correlation between input and output for correct prediction, we make a gradual adjustments in the network. Rather than just trying to figure out direct correlation from input to output, we try to look for an intermediate state which has limited correlation with the output and based upon it we try to predict output. We call these intermediate state(s) as _layer(s)_.\n",
    "Hence, multiple hidden layers are added to neural networks between input layer and output layer to find relationships in complex scenarios. \n",
    "\n",
    "ü•∑ **These hidden layers are nothing but different combination of weights per scenario.** ü•∑\n",
    "\n",
    "_Output of one layer is feeded to next layer and so forth,_ till we reach to a final conclusion. üéØ\n",
    "\n",
    "This is also called **Forward Propagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü´∏ Let us now introduce a hidden layer with different weight combinations for above scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üìåNote: We are right now focussing on capturing linear relationships only. We will come to non-linearity later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing arbitary weights w.r.t input values\n",
    "        #    height weight  age greying\n",
    "ip_weights = [[0.15, 0.01, 0.02, 0.5],  # Weights for adult probability output\n",
    "              [0.2, 0.005, 0.01, 0.4]]  # Weights for overall health score output\n",
    "# introducing hidden layer of weights\n",
    "hd_weights = [[0.10, 0.07], # adult probablity\n",
    "              [0.1, 0.5]]   # health score\n",
    "\n",
    "# maintaining an array of weights\n",
    "weight_layer = [ip_weights, hd_weights]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ In the _'hd_weights'_ we have an added layer, which will take output from first layer as input and apply corresponding weights w.r.t output parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìå **Note:** The underlying calculation remains same. We just add one added step in our neural network, where we feed the predictions from first layer to the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "def normalize_inputs(input_data, min_values, max_values):\n",
    "    return [(input_data[i] - min_values[i]) / (max_values[i] - min_values[i]) for i in range(len(input_data))]\n",
    "\n",
    "# Weighted Sum function\n",
    "def w_sum(a, b):\n",
    "    if len(a) == len(b):\n",
    "        output = 0\n",
    "        for i in range(len(a)):\n",
    "            output += a[i] * b[i]  # Weighted sum of inputs\n",
    "    return output\n",
    "\n",
    "# Vector-Matrix multiplication function which calculates the weighted sum of inputs w.r.t to weights\n",
    "def vect_mat_mul(input, weights):\n",
    "    if len(input) == len(weights[0]):\n",
    "        output = [0, 0]  # Adjusted to 2 outputs\n",
    "        for i in range(len(weights)):  # Loop for each output\n",
    "            output[i] = w_sum(input, weights[i])  # Calculate weighted sum for each output\n",
    "    return output\n",
    "\n",
    "# Neural network\n",
    "def neural_network(input, weight_layer):\n",
    "    # First Layer: where we have inputs and first set of weights\n",
    "    hid = vect_mat_mul(input, weight_layer[0])\n",
    "    # Second Layer: where we have output of 'first layer' and second set of weights\n",
    "    pred = vect_mat_mul(hid, weight_layer[1])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining input parameters\n",
    "person_height = [5.5, 2.5, 6.4, 7, 2.3]\n",
    "person_weight = [150, 30, 180, 220, 25]\n",
    "person_age = [25, 4, 30, 35, 3]\n",
    "person_greying = [0.2, 0.0, 0.1, 0.3, 0.0]\n",
    "# Min values for [height, weight, age, hair greying]\n",
    "min_values = [2.3, 25, 3, 0.0]\n",
    "# Max values for [height, weight, age, hair greying]\n",
    "max_values = [7.0, 250, 100, 1.0]\n",
    "input = [person_height[0], person_weight[0], person_age[0], person_greying[0]]\n",
    "normalized_input = normalize_inputs(input, min_values, max_values)\n",
    "\n",
    "pred = neural_network(normalized_input, weight_layer)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Important:** \n",
    "If we look closely, we can see that we are scaling the pattern (intermediate output) corresponding to health score with the hidden weight corresponding to adult probablity.\n",
    "\n",
    "üëÅÔ∏è **Visualize:**\n",
    "\n",
    "Original input for first person:\n",
    "input = [5.5, 150, 25, 0.2]\n",
    "\n",
    "-> After normalization (approximate values)\n",
    "normalized_input = [0.68, 0.5, 0.22, 0.2]  # These are rough values based on your min/max\n",
    "\n",
    "-> First Layer Calculation:\n",
    "ip_weights = [\n",
    "    [0.15, 0.01, 0.02, 0.5],    # Weights for output 1\n",
    "    [0.2, 0.005, 0.01, 0.4]     # Weights for output 2\n",
    "]\n",
    "\n",
    "Output1 = (0.68 √ó 0.15) + (0.5 √ó 0.01) + (0.22 √ó 0.02) + (0.2 √ó 0.5)\n",
    "        = 0.102 + 0.005 + 0.0044 + 0.1\n",
    "        = 0.2114\n",
    "\n",
    "Output2 = (0.68 √ó 0.2) + (0.5 √ó 0.005) + (0.22 √ó 0.01) + (0.2 √ó 0.4)\n",
    "        = 0.136 + 0.0025 + 0.0022 + 0.08\n",
    "        = 0.2207\n",
    "\n",
    "-> After first layer we have: [0.2114, 0.2207]\n",
    "\n",
    "--> Second Layer Calculation:\n",
    "hd_weights = [\n",
    "    [0.10, 0.07],    # Weights for final adult probability\n",
    "    [0.1, 0.5]       # Weights for final health score\n",
    "]\n",
    "\n",
    "Final_Adult_Prob = (0.2114 √ó 0.10) + (0.2207 √ó 0.07)\n",
    "                 = 0.02114 + 0.015449\n",
    "                 = 0.036589\n",
    "\n",
    "Final_Health_Score = (0.2114 √ó 0.1) + (0.2207 √ó 0.5)\n",
    "                   = 0.02114 + 0.11035\n",
    "                   = 0.13149\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üôã‚Äç‚ôÇÔ∏è‚ùì **One may wonder why are we scaling output corresponding to health score w.r.t weight related to adult probablity in the hidden layer calculation?**\n",
    "\n",
    "- What we have from the output of first layer is not strictly \"preliminary adult assessment\" and \"preliminary health assessment\". We can think of them as _learned features_ or _intermediate patterns_ that network will use in future.\n",
    "- These learned features may contain useful information for the final prediction outcome. Hence, neural network takes them into consideration.\n",
    "\n",
    "ü•∏ Let us consider a real world analogy. If someone is an adult, he/she might be taller and more muscular. Similarly, height-to-weight ratio and muscle mass both indicate health.\n",
    "\n",
    "In case, we want to have strict seperation where first output only affects adult probability and second only affects health, we could have defined the hidden layer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented code\n",
    "# hd_weights = [\n",
    "#     [1.0, 0.0],    # Only use first feature for adult probability\n",
    "#     [0.0, 1.0]     # Only use second feature for health score\n",
    "# ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è However, this pre-defined clear seperation will severely limit the ability of the network to learn complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2Ô∏è‚É£ Learning the errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the earlier steps, we were able to make predictions but we were not comparing them with the actual results to judge the margin by which the predictions missed. This margin is called 'Error'.\n",
    "\n",
    "> The Learning part of a Machine Learning or a Deep Learning model is about figuring out how each weight played its part in creating error, so that it can be adjusted to get close to the actual output.\n",
    "\n",
    "Now that, we have a basic framework in place for setting up nodes and weights of a neural network, we get to the next part: \n",
    "‚è≠Ô∏è **\"How to set weights for accurate prediction?\"**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways of measuring error, however we focus on this one specifically, as it is one of the basic yet quite commonly used error capturing mechanism. The unqiue point about this mechanism is that it squares the error captured, so that big errors are amplified and smaller ones are reduced. \n",
    "> Note: This is one way of prioritizing errors, other mechanisms of measuring error will priortize differently.\n",
    "\n",
    "By squaring the error, numbers that are less than 1 get smaller, whereas numbers that are greater \n",
    "than 1 get bigger. Also, the error of each prediction is always positive yet they do not accidently cancel each other out while averaging them. Consider a scenario, where we had 2 datapoints with one having an error of 10 and other with an error of -10. The average error in this case would be zero.\n",
    "\n",
    "üí° _Idea is quite similar to parenting children, where parents tend to ignore smaller mistakes and pay attention to the critical ones._ \n",
    "\n",
    "‚õèÔ∏èThe formula for calculating Mean Squared Error (MSE) is:\n",
    "$MSE = \\frac{1}{n} \\sum_{i=1}^n (Y_i - \\hat{Y}_i)^2$\n",
    "\n",
    "It calculates the average squared difference between the predicted values and the actual values.\n",
    "\n",
    "The lower value of MSE indicates that the accurace of the model is better and higher value suggests the opposite. Once, we get the value of MSE, we try to adjust the weights of the model up and down,so that we get close to the actual output. However, this takes a considerable number of iteration and is often the case of Hit & Trial, also known as **Hot and Cold Learning**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hot and Cold Learning example:\n",
    "weight = 0.11\n",
    "input = 5.5\n",
    "actual_output = 0.82\n",
    "# the amount by which we will tweak the weights in either direction.\n",
    "step_amount = 0.001\n",
    "# Number of times iteration will run. Can be given any arbitrary number and played around.\n",
    "iteration_steps = 1101\n",
    "\n",
    "for iteration in range(iteration_steps):\n",
    "    prediction = input * weight\n",
    "    error = (actual_output - prediction) ** 2\n",
    "    print(\"Error:\" + str(error) + \" Prediction:\" + str(prediction))\n",
    "    # After predicting, we make prediction two more times, first by slightly lowering the weights\n",
    "    # and then again by slightly increasing the weight by the 'step_amount'.\n",
    "    up_prediction = input * (weight + step_amount)\n",
    "    up_error = (actual_output - up_prediction) ** 2\n",
    "    down_prediction = input * (weight - step_amount)\n",
    "    down_error = (actual_output - down_prediction) ** 2\n",
    "    # IN CASE: lowering the weight reduces error, we reduce the weight\n",
    "    if (down_error < up_error):\n",
    "        weight = weight - step_amount\n",
    "    # IN CASE: increasing the weight reduces error, we increase the weight\n",
    "    if (down_error > up_error):\n",
    "        weight = weight + step_amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚òëÔ∏èCheck: The predicted value in the last iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è The above exercise has 2 problems:\n",
    "1. It is very hard to guess the 'iteration_steps' to reach optimal weights. Also, it takes way too many iterations.\n",
    "2. Choosing the exact 'step_amount' is next to impossible. \n",
    "The above example is a very carefully choose one with correct configurations. In reality, it is very hard to start these two variables with correct values in one go.\n",
    "\n",
    "‚è≠Ô∏è Next, we try to optimize this process by adapting to a superior form of learning: **Gradient Descent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "üí°Gradient Descent allows to calculate both **'direction and amount'** of weight adjustment in a single step to reduce the error.\n",
    "\n",
    "$$\n",
    "direction\\_and\\_amount = (pred - actual\\_output) \\cdot input\n",
    "$$\n",
    "\n",
    "$(pred - actual\\_output)$ is often called as **$pure\\ error$**. So we can rewrite:\n",
    "\n",
    "$$\n",
    "direction\\_and\\_amount = pure\\ error \\cdot input\n",
    "$$\n",
    "\n",
    "ü´µThe *pure error* indicates the direction and the amount by which the actual target is missed. Pure error, by itself, is not capable of making optimal modification to weight. By multiplying the pure error by the input value, we translate the pure error into the absolute amount by which we want to change the weight. This covers three major edge cases where pure error falls short: **_stopping, scaling, and negative reversal_**.\n",
    "\n",
    "üî¥ **_Stopping_** is simplest of the three edge cases covered. Suppose, we have an input value as 0, then there is no point of adjusting the corresponding weight. _If there is no input to TV, what is the point of changing the channel?_ ü§î \n",
    "\n",
    "Hence, $if \\ input = 0; direction\\_and\\_amount = 0$. \n",
    "\n",
    "‚úñÔ∏è Next is **_Scaling_**, which adjusts magnitude of weight update based upon the size of the input. If an input size is large, this means it will play a larger role role in prediction, hence, corresponding weight to it should be updated more, and vice versa. \n",
    "\n",
    "‚óÄÔ∏è **_Negative Reversal_** means flipping the sign of pure error when the input is negative. When an input is negative, increasing the weight decreases the output and this is taken care automatically by the above step as the direction of weight adjustment is reversed.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Example:\n",
    "weight = 0.11\n",
    "actual_output = 0.82\n",
    "input = 5.5\n",
    "# Number of times iteration will run. Can be given any arbitrary number and played around.\n",
    "iteration_steps = 20\n",
    "for iteration in range(iteration_steps):\n",
    "    pred = input * weight\n",
    "    error = (pred - actual_output) ** 2\n",
    "    # Calculation of Gradient Descent\n",
    "    direction_and_amount = (pred - actual_output) * input\n",
    "    weight = weight - direction_and_amount\n",
    "    print(\"Error:\" + str(error) + \" Prediction:\" + str(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üîë **Backpropagation:** \n",
    "Look for this step in the program aboveüëÜ ->\n",
    "\n",
    "`weight = weight - direction_and_amount`\n",
    "\n",
    "This step is called _backpropagation_, where we adjust the weights based upon the feedback recieved at output layer. Ofcourse, what we are seeing here is in its most simplified form without any intermediate layers. **_As we add more layers in between, we backpropagate this feedback from one layer to other by attributing the error to the weights and adjusting them._**\n",
    "\n",
    "More on this topic later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üëÅÔ∏èObserve: How the predicted values are oscillating around the actual value (target). Sometimes, the value overshoots the target value and sometimes it undershoots."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è In the example above, we oversimplified the process of Gradient Descent and overlooked the side-effects caused by this oversimplification process. When we scale 'pure error' by multiplying it with the input value, there is a strong possibility that it can overshoot beyond the optimal point. \n",
    "\n",
    "In the early phase of training a neural network, the weights are far from the optimal solution, and the gradient descent provides only a rough direction to reduce the loss. Hence, scaling helps the model cover more ground in the loss landscape quickly in order to find regions of lower error.\n",
    "\n",
    "However, as training progresses and the model gets closer to the optimal solution, the gradients become smaller since the loss function flattens near the minimum.\n",
    "\n",
    "ü§î _Imagine rolling down a ball down the slope. If the ball's speed is high near the bottom, it will climb next the slope upwards._\n",
    "![image.png](attachment:image.png)\n",
    "<small>image credit: https://amrita.olabs.edu.in/</small>\n",
    "\n",
    "‚õèÔ∏è To counter this challenge, we introduce a new parameter called '**learning rate**', which is basically meant to keep in check the size of steps we take towards the _minimum error checkpost_ ‚õ≥. There is no rocket science here, we just multiply the updated weight with a fraction to keep its size in check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîôLet us take a step back: What is Gradient Descent?\n",
    "\n",
    "üñáÔ∏èThe word _gradient_ in english means _'a part sloping upward or downward'_. When we are trying to reduce or eliminate error, we are trying to reach a stable ground truth, similar to a ball rolling down the hill or a man trying to climb up. In order to either reach the bottom or top, both ball or man may take longer strides initially but as they reach closer to the target, steps will get smaller and smaller to avoid overshooting it. In this context, the gradient is a vector that contains the direction of the steepest step a ball or man can take and how long that step should be.\n",
    "\n",
    "üí≠ Revise --> slope of a line is represented by the formulae:\n",
    "$$\n",
    "y = mx + b\n",
    "$$\n",
    "\n",
    "In order to observe steepness of a slope, we start from any starting point and then find derivative to measure the steepness and direction of the slope. This is what we do in linear regression problems to find the line of best fit. \n",
    "\n",
    "> üèπ The aim of Gradient Descent is to minimize the error (or loss function) between predicted value and the actual ouput. And in order to do so, it requires a **_direction_ and _learning rate_** (the size of steps to be taken to reach the minimum point). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚Ü™Ô∏è Revisit: Gradient Descent\n",
    "\n",
    "In the simplified introduction of Gradient Descent, we were calculting both _direction_ and _amount_ of change in one step only. We now break this process into small steps. First, we calculate the raw error which measures how much the prediction was off from the target value and then we compute the _direction_ and _amount_ a weight needs to change to reduce error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Example:\n",
    "weight = 0.02\n",
    "actual_output = 0.82\n",
    "input = 5.5\n",
    "alpha = 0.027 # learning rate, try changing this value to see how quickly prediction changes\n",
    "# Number of times iteration will run. Can be given any arbitrary number and played around.\n",
    "iteration_steps = 4\n",
    "for iteration in range(iteration_steps):\n",
    "    prediction = input * weight\n",
    "    error = (prediction - actual_output) ** 2\n",
    "    pure_error = prediction - actual_output\n",
    "    weight_delta = input * pure_error    \n",
    "    #weight = weight - weight_delta\n",
    "    weight = weight - (weight_delta * alpha) # incorporating learning rate, play around\n",
    "    print(\"Error:\" + str(error) + \" Prediction:\" + str(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚¨ÜÔ∏èIn the above computation, we have introduced a small but critical parameter called **_learning rate_**. It is the scaling factor that controls the magnitude of the weight updates to ensure that the network learns at a controlled and stable pace. By multiplying _weight_delta_ with learning rate, we adjust the step size of the weight update. \n",
    "\n",
    "But wait! what is this _weight_delta_ ? ü§∑‚Äç‚ôÇÔ∏è\n",
    "\n",
    "üí° _weight_delta_ is the measure of how much a weight was responsible in the error. It is calculated by scaling the 'pure error'(gap between actual and predicted value) with the input value. This step is similar to the earlier step to account for scaling, negative reversal, and stopping.\n",
    "\n",
    "üîç If we look closely, the input value and the actual output value is fixed for a neural network. At this point, it boils down to tweaking the weight and learning rate to reduce the error. Hence, we can define an error as a function of weight. \n",
    "\n",
    "üìà Let us take the above example and plot the error function:\n",
    "$$\n",
    "error=((0.5‚ãÖweight)‚àí0.8)^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set Seaborn style for a clean look\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Define the weight range and compute error\n",
    "weights = np.linspace(0, 0.25, 500)  # Range for weight values\n",
    "errors = ((5.5 * weights) - 0.82) ** 2\n",
    "\n",
    "# Identify the special points\n",
    "weight_zero = 0\n",
    "error_at_weight_zero = ((5.5 * weight_zero) - 0.82) ** 2\n",
    "\n",
    "# Find the weight that minimizes the error\n",
    "optimal_weight = 0.82 / 5.5  # Solve 0.5 * weight - 0.8 = 0\n",
    "min_error = ((5.5 * optimal_weight) - 0.82) ** 2  # Should be 0\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(weights, errors, label=\"Error vs Weight\", color=\"blue\", linewidth=2)\n",
    "\n",
    "# Highlight the special points\n",
    "plt.scatter(weight_zero, error_at_weight_zero, color=\"red\", s=100, label=\"Weight = 0\", edgecolor=\"black\")\n",
    "plt.scatter(optimal_weight, min_error, color=\"green\", s=100, label=\"Error = 0 (Optimal Weight)\", edgecolor=\"black\")\n",
    "\n",
    "# Add annotations for clarity\n",
    "plt.text(weight_zero, error_at_weight_zero + 0.01, f\"({weight_zero}, {error_at_weight_zero:.2f})\", color=\"red\")\n",
    "plt.text(optimal_weight, min_error - 0.03, f\"({optimal_weight:.2f}, {min_error})\", color=\"green\")\n",
    "\n",
    "# Add titles and labels\n",
    "plt.title(\"Error as a Function of Weight\", fontsize=14)\n",
    "plt.xlabel(\"Weight\", fontsize=12)\n",
    "plt.ylabel(\"Error\", fontsize=12)\n",
    "plt.axhline(0, color='black', linewidth=0.7, linestyle='--')  # Horizontal reference line\n",
    "plt.axvline(0, color='black', linewidth=0.7, linestyle='--')  # Vertical reference line\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(True, alpha=0.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÅÔ∏è **Observations:** \n",
    "- When weight is zero, the model prediction is quite far away from the actual output.\n",
    "- At weight 0.15, prediction matches the target and error is zero. This is optimal weight.\n",
    "- Notice the convergence of gradient at weight 1.6\n",
    "- If we draw a slope before 0.15, derivative will be negative, which basically means: if the slope is   negative, increasing the weight decreases the error.\n",
    "- If we draw a slope after 0.15, derivative will be positive, which basically means: if the slope is   positive, increasing the weight increases the error.\n",
    "\n",
    "üîë Derivative is the relationship or marker of sensitivity between two variables in a function. \n",
    "\n",
    "üìå **Gradient descent uses the derivative to iteratively adjust the weight by moving in the direction of negative slope, hence the name 'Gradient Descent'.**\n",
    "_If the evidence at hand consistently points towards the truth, we give it more weight or else if it suggests otherwise, we consider it unreliable and give it less weightage._\n",
    "\n",
    "‚è≠Ô∏è So far we have been using Gradient Descent with single input and output, now we raise the complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü´∏Gradient Descent with multiple inputs\n",
    "\n",
    "In the earlier step, for updating weight of a single input to get the prediction closer to actual output, we calculated its corresponding '**_weight_delta_**' by scaling the 'pure error' with the input value, which gave us a derivative based estimate about the direction and amount we should be adjusting the weight.\n",
    "\n",
    "In the case of multiple inputs, we follow the same process. Since, each weight is associated with a particular input value, in order to calculate the corresponding _weight_delta_, we scale the 'pure error' with each input value.\n",
    "\n",
    "‚è≠Ô∏è For clarity, let us expand the original example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisiting the neural network code which we had written earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Sum function\n",
    "def w_sum(a, b):\n",
    "    if (len(a) == len(b)):\n",
    "        output = 0\n",
    "    for i in range(len(a)):\n",
    "        output += (a[i] * b[i])\n",
    "    return output\n",
    "# Setting up Neural Network\n",
    "def neural_network(input,weights):\n",
    "    pred = w_sum(input,weights)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate derivative wrt each input and pure error\n",
    "def ele_mul(scalar, vector):\n",
    "    out = [0,0,0,0]\n",
    "    for i in range(len(out)):\n",
    "        out[i] = vector[i] * scalar\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent example with multiple inputs\n",
    "person_height = [5.5, 2.5, 6.4, 7, 2.3]\n",
    "person_weight = [150, 30, 180, 220, 25]\n",
    "person_age = [25, 4, 30, 35, 3]\n",
    "person_greying = [0.2, 0.0, 0.1, 0.3, 0.0]\n",
    "alpha = 0.01 #learning rate\n",
    "weights = [0.02, 0.001, 0.005, 0.1] # initial weight for height weight  age greying respectively\n",
    "input = [person_height[0],person_weight[0],person_age[0], person_greying[0]] # first set of input\n",
    "\n",
    "win_or_lose_binary = [1, 0, 1, 1, 0] \n",
    "true = win_or_lose_binary[0] # Actual result for first set of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = neural_network(input,weights)\n",
    "error = (pred - true) ** 2\n",
    "pure_error = pred - true\n",
    "weight_deltas = ele_mul(pure_error, input)\n",
    "\n",
    "# Adjusting weight after considering learning rate 'alpha'\n",
    "for i in range(len(weights)):\n",
    "    weights[i] -= alpha * weight_deltas[i]\n",
    "    \n",
    "print(\"Weights:\" + str(weights))\n",
    "print(\"Weight Deltas:\" + str(weight_deltas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make neural networks learn by running few iterations\n",
    "# weight reset\n",
    "weights = [0.02, 0.001, 0.005, 0.1] # initial weight for height weight  age greying respectively\n",
    "alpha = 0.0008 #learning rate\n",
    "\n",
    "for iter in range(4):\n",
    "\n",
    "    pred = neural_network(input,weights)\n",
    "\n",
    "    error = (pred - true) ** 2\n",
    "    delta = pred - true\n",
    "\n",
    "    weight_deltas=ele_mul(delta,input)\n",
    "\n",
    "    print(\"Iteration:\" + str(iter+1))\n",
    "    print(\"Pred:\" + str(pred))\n",
    "    print(\"Error:\" + str(error))\n",
    "    print(\"Delta:\" + str(delta))\n",
    "    print(\"Weights:\" + str(weights))\n",
    "    print(\"Weight_Deltas:\")\n",
    "    print(str(weight_deltas))\n",
    "    print(\n",
    "    )\n",
    "\n",
    "    for i in range(len(weights)):\n",
    "        weights[i]-=alpha*weight_deltas[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÅÔ∏è **Key Observation:**\n",
    "- Weight corresponding to the input 'weight' is updated significantly compared to other weights.\n",
    "\n",
    "üîë Since, the input 'weight' value for first set of input is comparatively larger than values of other inputs i.e. '150' in comparison to '5.5', '25' and '0.2'; the weight corresponding to this input gets away with significant chunk of updates while rest of the other weights are neglected to some extent. \n",
    "\n",
    "üî¶ Hint: We scale 'pure error' with input value to get derivatives corresponding to each input. The weight associated with the largest input dominates the learning process since its gradient is the largest.\n",
    "\n",
    "üö® However, this is not an ideal scenario in most use cases, as we had discussed earlier as well. For example, area of a property might be 1500 sqft, but the price of the same property might have a figure like $1,00,000. In this case, price factor will dominate the neural network learning process, which is not accepted.\n",
    "\n",
    "‚õèÔ∏è Therefore, there arises the need to adjust the scales of input values through the process of **Normalization** (_explained in earlier section_), so that all input values take equal part in the learning process of the neural network. This makes sure that gradients are calculated uniformly and so do weight adjustments. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü´∏ Gradient Descent: Multiple Inputs and Multiple Outputs\n",
    "\n",
    "In the first section, we have already built a network with multiple inputs and outputs. We start from there and introduce gradient descent method to make our network learn better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector-Matrix multiplication function which calculates the weighted sum of inputs w.r.t to weights\n",
    "def vect_mat_mul(input, weights):\n",
    "    if len(input) == len(weights[0]):\n",
    "        output = [0, 0]  # Adjusted to 2 outputs\n",
    "        for i in range(len(weights)):  # Loop for each output\n",
    "            # Calculate weighted sum for each output\n",
    "            output[i] = w_sum(input, weights[i])\n",
    "    return output\n",
    "\n",
    "# Setting up Neural Network\n",
    "def neural_network(input, weights):\n",
    "    pred = vect_mat_mul(input, weights)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up input and output parameters\n",
    "# defining input parameters\n",
    "person_height = [5.5, 2.5, 6.4, 7, 2.3]\n",
    "person_weight = [150, 30, 180, 220, 25]\n",
    "person_age = [25, 4, 30, 35, 3]\n",
    "person_greying = [0.2, 0.0, 0.1, 0.3, 0.0]\n",
    "\n",
    "#            height weight  age greying\n",
    "ip_weights = [[0.15, 0.01, 0.02, 0.5],  # Weights for adult probability output\n",
    "              [0.2, 0.005, 0.01, 0.4]]  # Weights for overall health score output\n",
    "# learning rate\n",
    "alpha = 0.01\n",
    "# first set of input   \n",
    "input = [person_height[0],person_weight[0],person_age[0], person_greying[0]] \n",
    "# actual output for adult probablity and health score\n",
    "actual_output  = [0.82, 0.90]\n",
    "\n",
    "pred = neural_network(input,ip_weights)\n",
    "\n",
    "error = [0, 0] \n",
    "delta = [0, 0]\n",
    "\n",
    "# now that we have multiple output, we consider each outcome and its relation to each input\n",
    "for i in range(len(actual_output)):\n",
    "    error[i] = (pred[i] - actual_output[i]) ** 2\n",
    "    delta[i] = pred[i] - actual_output[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üñêÔ∏è This code is demonstrating a single step of training our neural network. We can play with it and add iterations to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Weight Delta calculation\n",
    "def outer_prod(a, b):\n",
    "    \n",
    "    # just a matrix of zeros\n",
    "    out = np.zeros((len(a), len(b)))\n",
    "\n",
    "    for i in range(len(a)):\n",
    "        for j in range(len(b)):\n",
    "            out[i][j] = a[i] * b[j]\n",
    "    return out\n",
    "# Note: The function above will create a 2x4 matrix in our case. 2 outputs and 4 corresponding inputs.\n",
    "weight_deltas = outer_prod(delta,input)\n",
    "\n",
    "# Weight Adjustment\n",
    "for i in range(len(ip_weights)):\n",
    "    for j in range(len(ip_weights[0])):\n",
    "        ip_weights[i][j] -= alpha * weight_deltas[i][j]\n",
    "\n",
    "ip_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚è≠Ô∏è Till now, we have been capturing input and output data points correspoding to a single example. Now this the time we raise the complexity and make neural networks learn all the training data set at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü´∏Training Entire Dataset: Representing dataset in the form of a matrix\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# existing input points\n",
    "person_height = [5.5, 2.5, 6.4, 7, 2.3]\n",
    "person_weight = [150, 30, 180, 220, 25]\n",
    "person_age = [25, 4, 30, 35, 3]\n",
    "person_greying = [0.2, 0.0, 0.1, 0.3, 0.0]\n",
    "\n",
    "weights = np.array([0.05, 0.2, 0.07, 0.1])\n",
    "alpha = 0.001\n",
    "# representing inputs in proper data set format. Visualize preparing reports in Excel.\n",
    "\n",
    "#                     height weight age hair_grey                           output:adult/child\n",
    "input_data = np.array( [[ 5.5, 150, 25, 0.2 ], # first set of input values  | 1 | adult\n",
    "                        [ 2.5, 30,  4,  0.0 ], # second set of input values | 0 | child\n",
    "                        [ 6.4, 180, 30, 0.1 ], # and so on..                | 1 | adult\n",
    "                        [ 7.0, 220, 35, 0.3 ], #                            | 1 | adult\n",
    "                        [ 2.3, 25,  3,  0.0 ], ] ) #                        | 0 | child\n",
    "\n",
    "adult_vs_child = np.array( [ 1, 0, 1, 1, 0 ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üõó Now that we have represented input and output data into matrices form (by help of NumPy), we start training our neural network with one set of inputs at a time. \n",
    "- So, over a defined iteration (number of steps for training), the network will take first set of inputs, will try to predict, calculate the weight delta and update weights.\n",
    "- This process will continue for other data sets sequentially.\n",
    "- Entire process will be repeated for a number of times (iteration) with a goal to find optimal weight configuration for the dataset provided.\n",
    "\n",
    "üëÜ The above defined process is called **_Stochastic gradient descent_**. There are other variations as well such as _Full gradient descent_ which updates weight one dataset at a time or _Batch gradient descent_ which updates weight after 'n' number of examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = input_data[0] \n",
    "goal_prediction = adult_vs_child[0]\n",
    "for iteration in range(40):\n",
    "    total_error = 0\n",
    "    for row_index in range(len(adult_vs_child)):\n",
    "        input = input_data[row_index]\n",
    "        goal_prediction = adult_vs_child[row_index]\n",
    " \n",
    "        prediction = input.dot(weights)\n",
    "        \n",
    "        error = (goal_prediction - prediction) ** 2\n",
    "        total_error += error\n",
    "        \n",
    "        delta = prediction - goal_prediction\n",
    "        weights = weights - (alpha * (input * delta))\n",
    "        print(\"Updated weights:\", weights)\n",
    "        print(\"Prediction:\" + str(prediction))\n",
    "    print(\"Error:\" + str(total_error) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3Ô∏è‚É£ Capturing Complex Relationships\n",
    "\n",
    "üëâ Here's what we have done so far:\n",
    "- Majorly we have been dealing with a simple network, which is basically a **_single layer perceptron_** with input datasets and output.\n",
    "- We defined error as a function of weight, which means we adjust weight to reduce error.\n",
    "- We performed above adjustment for some iterations **_hoping_** to optimally adjust weight values to produce minimal error.\n",
    "\n",
    "üñêÔ∏è _Hoping?_ \n",
    "\n",
    "ü§´ _Confession:_ Yes, in the back of our mind we kind of presumed that input dataset is going to have some kind of correlation with the output. We chose simplest of the scenarios we could to understand the basic functioning of the network components. We did not consider scenarios where there is going to be abolutely no correlation between input and output datasets, which is mostly the case in real world situations. We just briefly introduced the concept of **_Hidden Layers_** and **_backpropagation_** and moved on. In reality, there is hardly a _linear relationship_ between input and output (<small>_Remember the dot product we have been calculating all along_</small>). Although, we all know this for a fact, yet we chose to ignore this for the sake of brevity.\n",
    "\n",
    "‚è≠Ô∏è Now that we know how different components of a neural network are and how they function with respect to each other, it is time to make things right. We now prepare our network to capture complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üï∏ Capturing Non-Linearity\n",
    "\n",
    "üåå We live in a multi-dimensional world. Let us pick a random entity, say a loaf of bread. We can associate multiple dimensions with it for analytics purpose. For example, we can note down length, breadth, and thickness of the bread, which corresponds to its size. Next, we can take a note of the amount of wheat flour and emulsifiers used in the baking process, along with the temperature it has been baked at. The resultant breads might vary in taste and color. However, none of the attributes mentioned earlier are linearly associated with the resultant properties of the bread. If given a task to predict taste based on the attributes mentioned earlier, there will not exist any linear algebraic function which will capture this relationship. _A linear function is one which produces straight line outputs when plotted against available inputs._\n",
    "\n",
    "`f(x) = W.x + b`\n",
    "\n",
    "üìê Here, _f(x)_ is a linear function where relationship between input attributes and output is a straight line in multi-dimensional space. W represents the importance given to an input feature. Generally, _(W.x)_ is called weighted sum, which combines all inputs into a single value that reflects the contribution of all features for a given neuron. Since, the weighted sum is constrained to pass through the origin (0, 0) in the input-output space, we add a bias which allows the network to shift the decision boundary.\n",
    "\n",
    "ü™¢ However, real world data more than often involves non-linear relationships. And to make predictions based on real world data, the machine learning models based on neural networks have to utilize non-linearity to capture complex relationships. In Mathematical terms, a linear model can only represent straight lines of planes:\n",
    "\n",
    "`f(x1, x2) = W1.x1 + W2.x2 + b`\n",
    "\n",
    "üåê But to capture complex relations which involve complex patterns, curves, or interactions, non-linearity has to be captured by a model. For example, in a binary classification problem, classes cannot be divided by a straight line. Or, in XOR problem, the target output is dependent on combination of inputs rather than direct relation between input and output. More complex problems like image recognition requires recognizing patterns like edges, shapes, and textures, which are non-linearly related to the pixel values. Due to market complexities, stock market prediction based on historical data cannot be linear. To capture non-linearity, neural networks make use of activation functions, represented as:\n",
    "\n",
    "`f(x1,x2) = f'(W1.x1 + W2.x2 + b)`\n",
    "\n",
    "This activation function _f'_ can produce outputs which can capture curves, kinks, etc. Depending upon the problem statement, the suitable activation function(s) is/are picked by data scientists and most of the times it is a case of trial and error. \n",
    "\n",
    "üîß The **_sigmoid activation_** function is often used to squash the output of a neuron into a bounded range, specifically between 0 and 1. This property makes it well-suited for tasks where the network‚Äôs output needs to represent probabilities or binary classifications. By confining the output values within a limited range, the sigmoid function ensures that predictions remain within a meaningful and interpretable range. \n",
    "\n",
    "> üöß **Caution:** While the sigmoid activation function does introduce some level of nonlinearity, it is less effective at combating the vanishing gradient. The _vanishing gradient problem occurs_ when gradients become very small during backpropagation, leading to negligible updates to the network‚Äôs parameters. This issue is particularly problematic for deep networks with many layers, as the gradients can diminish exponentially as they are propagated backward. When gradients are small, the network updates its parameters at a sluggish pace, effectively impeding the model‚Äôs ability to learn from the data. This can result in networks that fail to capture complex patterns and fail to generalize well to new data. Due to the vanishing gradient problem, using the sigmoid activation function between hidden layers is generally not recommended, especially for deep networks. more modern activation functions, such as ReLU and its variants (Leaky ReLU, Parametric ReLU), have become more popular choices for hidden layers. These functions address the vanishing gradient issue more effectively and enable faster convergence.\n",
    "\n",
    "‚è≠Ô∏è Now we enhance our previous code to capture non-linearity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sigmoid activation function and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Initialize inputs and target outputs\n",
    "input_data = np.array([\n",
    "    [5.5, 150, 25, 0.2],  # height, weight, age, hair_grey\n",
    "    [2.5, 30, 4, 0.0],\n",
    "    [6.4, 180, 30, 0.1],\n",
    "    [7.0, 220, 35, 0.3],\n",
    "    [2.3, 25, 3, 0.0]\n",
    "])\n",
    "adult_vs_child = np.array([1, 0, 1, 1, 0])\n",
    "# # Min values for [height, weight, age, hair greying]\n",
    "# min_values = [2.3, 25, 3, 0.0]\n",
    "# # Max values for [height, weight, age, hair greying]\n",
    "# max_values = [7.0, 250, 100, 1.0]\n",
    "# Normalize the inputs\n",
    "min_values = np.min(input_data, axis=0)\n",
    "max_values = np.max(input_data, axis=0)\n",
    "normalized_input = (input_data - min_values) / (max_values - min_values)\n",
    "\n",
    "# Initialize weights and hyperparameters\n",
    "weights = np.array([0.05, 0.2, 0.07, 0.1])  # Single layer\n",
    "alpha = 0.01\n",
    "epochs = 40\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_error = 0\n",
    "    for i in range(len(adult_vs_child)):\n",
    "        input = normalized_input[i]\n",
    "        target = adult_vs_child[i]\n",
    "\n",
    "        # Forward pass\n",
    "        weighted_sum = np.dot(input, weights)\n",
    "        prediction = sigmoid(weighted_sum)\n",
    "\n",
    "        # Error calculation\n",
    "        error = (target - prediction) ** 2\n",
    "        total_error += error\n",
    "\n",
    "        # Backpropagation: compute gradient\n",
    "        delta = (target - prediction) * sigmoid_derivative(prediction)\n",
    "\n",
    "        # Weight update\n",
    "        weights += alpha * input * delta\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Total Error: {total_error}\")\n",
    "\n",
    "print(\"Final weights:\", weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÅÔ∏è **Observations:**\n",
    "- The sigmoid activation function has been applied on weighted sum to transform it into a value between 0 and 1. When weighted sum is is very large (positive), sigmoid becomes 1 anc conversely it approaches 0. \n",
    "    - For classification problems (like adult vs. child), the sigmoid output can be interpreted as a probability.\n",
    "    - It also helps the network to approximate non-linear relationships in the data and thus allowing the model to solve more complex problems.\n",
    "- The Sigmoid derivative tells us how much sigmoid function changes when weighted sum changes.\n",
    "- Since sigmoid squashes the output between 0 and 1, to capture the activation function's role in the sensitivity of the output to changes in the input weights, we scale the previous delta calculation with _sigmoid derivative_.\n",
    "    - We are basically propagating error backward and scaling it by how much each weight contributes to the error.\n",
    "    - This means if the derivative (change) is small, we update the weight less and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚Ü™Ô∏è Revisit: ü•∑ Hidden Layers ü•∑\n",
    "\n",
    "üßê **But why?**\n",
    "\n",
    "Because we do not live in a simple world. World is complex, full of layers. It may sound laughable at the moment but let us look at some analogies.\n",
    "\n",
    "- To present the entire context about Neural Networks, I chose Letters ‚Üí (formed) Words ‚Üí (formulated) Phrases ‚Üí (completed) Sentences ‚Üí (presented) Context. \n",
    "- When we watch a picture on a device, it is composed of Pixels ‚Üí Edges ‚Üí Shapes ‚Üí Objects ‚Üí Scene.\n",
    "- When we listen to a song, it is composed of Sounds ‚Üí Phonemes ‚Üí Words ‚Üí Sentences ‚Üí Meaning.\n",
    "\n",
    "üôÉ It would be really absurd if we think a single layer perceptron model can capture complexities of the world.\n",
    "\n",
    "ü§î **How multiple layer helps?**\n",
    "Since, in real world, it is often the case that input does not correlate directly with the output, we introduce intermediate layer(s), which act as intermediary state which has some correlation with respect to output. \n",
    "\n",
    "Say, we want to classify an image as cat or dog. A single layer may be able to detect simple features like ear, nose, etc,. but it will not sufficient to distinguish how these features combine to form a cat or dog. The model will be inaccurate. If we introduce multiple layers in the model, we can divide the task to different layers. First layer will detect edges and curves, second layer will combine edges into parts and subsequent layer(s) will eventually combine parts into an object which can be matched with the label of cat or dog. \n",
    "\n",
    "‚è≠Ô∏è Now we incorporate this in our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Layer Neural Network with Backpropagation\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize inputs and target outputs\n",
    "input_data = np.array([\n",
    "    [5.5, 150, 25, 0.2],  # height, weight, age, hair_grey\n",
    "    [2.5, 30, 4, 0.0],\n",
    "    [6.4, 180, 30, 0.1],\n",
    "    [7.0, 220, 35, 0.3],\n",
    "    [2.3, 25, 3, 0.0]\n",
    "])\n",
    "adult_vs_child = np.array([1, 0, 1, 1, 0])\n",
    "\n",
    "# Normalize the inputs\n",
    "normalized_input = (input_data - min_values) / (max_values - min_values)\n",
    "\n",
    "# Initialize weights for input-to-hidden and hidden-to-output\n",
    "weights_input_hidden = np.random.rand(4, 3)  # 4 inputs -> 3 hidden nodes\n",
    "print(weights_input_hidden)\n",
    "weights_hidden_output = np.random.rand(3, 1)  # 3 hidden nodes -> 1 output\n",
    "print(weights_hidden_output)\n",
    "# Hyperparameters\n",
    "alpha = 0.1 #learning rate\n",
    "epochs = 40 #iterations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÅÔ∏è **Observations:**\n",
    "\n",
    "- We are introducing a 3-node hidden layer between 4-node input layer and 1-node output layer.\n",
    "- Since, each feature of input layer connects to every node in the hidden layer, so there are [4x3=12]weights. Hence, `weights_input_hidden` has a shape of [4x3]. \n",
    "- Weights are initialized so that each weight starts with a slightly different value.\n",
    "- Input has been transposed to 4x1 (from 1x4) so that it matches shape of the weights matrix dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    total_error = 0\n",
    "    for i in range(len(normalized_input)):\n",
    "        input = normalized_input[i].reshape(-1, 1)  # Column vector\n",
    "        target = adult_vs_child[i]\n",
    "\n",
    "        # Forward pass\n",
    "        hidden_layer_input = np.dot(input.T, weights_input_hidden)  # Input to hidden layer\n",
    "        hidden_layer_output = sigmoid(hidden_layer_input)  # Apply activation function\n",
    "\n",
    "        final_layer_input = np.dot(hidden_layer_output, weights_hidden_output)  # Hidden to output layer\n",
    "        prediction = sigmoid(final_layer_input)  # Apply activation function\n",
    "\n",
    "        # Error calculation\n",
    "        error = (target - prediction) ** 2\n",
    "        total_error += error\n",
    "\n",
    "        # Backpropagation\n",
    "        # Output layer delta\n",
    "        delta_output = (target - prediction) * sigmoid_derivative(prediction)\n",
    "\n",
    "        # Hidden layer delta\n",
    "        delta_hidden = delta_output.dot(weights_hidden_output.T) * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "        # Update weights\n",
    "        weights_hidden_output += alpha * hidden_layer_output.T.dot(delta_output)\n",
    "        weights_input_hidden += alpha * input.dot(delta_hidden)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Total Error: {total_error}\")\n",
    "\n",
    "print(\"Final weights (input to hidden):\", weights_input_hidden)\n",
    "print(\"Final weights (hidden to output):\", weights_hidden_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÅÔ∏è **Observations:**\n",
    "- The prediction is computed layer by layer. \n",
    "    - From input to hidden layer, Weighted Sum is calculated by `np.dot(input.T, weights_input_hidden)`.\n",
    "    - From hidden to output layer, Weighted Sum is calculated by `np.dot(hidden_layer_output, weights_hidden_output)`.\n",
    "- The sigmoid function has been applied in both steps to ensure non-linearity.\n",
    "- Weights are being updated layer by layer using _backpropagation_.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
